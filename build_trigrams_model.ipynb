{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2376ee9c",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from utils import sentences_by_lang, languages, get_trigrams_sets, encode\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a802b261",
   "metadata": {},
   "source": [
    "### Przygotowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e608439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytujemy plik csv tworząc DF z dwoma kolumanmi \"lang\" oraz \"sentence\"\n",
    "csv_file = pd.read_csv('sentences.csv', on_bad_lines='skip', sep='\\t', index_col=0, names=[\"lang\", \"sentence\" ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15c282da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrujemy tabelę, zostawiamy tylko wspierane języki. Dla każdego języka zostawiamy SENTENCES_BY_LANG zdań.\n",
    "dataset = csv_file[csv_file['lang'].isin(languages)]\n",
    "results = pd.DataFrame(columns=[\"lang\",\"sentence\"])\n",
    "for l in languages:\n",
    "    ds = dataset[dataset[\"lang\"] == l].sample(sentences_by_lang)\n",
    "    results = pd.concat([results, ds])\n",
    "results[\"sentence\"] = results[\"sentence\"].str.lower() # pomijamy wielkość liter, aby nie traktować osobno np. \"He\" i \"he\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca01e166",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trigrams, lang_trigrams = get_trigrams_sets(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219089b3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a380bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tworzymy bag of words, nie wykorzystujemy binarnego bag of words ponieważ trigramy w zdaniu mogą się powtórzyć i stracilibyśmy tę informację.\n",
    "# Wadą BoW jest fakt, że każdy trigram jest tak samo ważny, ale w naszym problemie to nie przeszkadza.\n",
    "dic = dict()\n",
    "for i,t in enumerate(all_trigrams):\n",
    "    dic[t]=i\n",
    "\n",
    "vectorizer = CountVectorizer(vocabulary=dic, ngram_range=(3,3), analyzer=\"char\") #ngram_range bierzemy tylko trigramy, analyzer bierzemy pod uwagę znaki,// char_wb nie zliczało poprawnie kolumn\n",
    "\n",
    "with open('count_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "train_sentences = results[\"sentence\"]\n",
    "train_langs = results[\"lang\"]\n",
    "X = vectorizer.fit_transform(train_sentences)\n",
    "train_features = pd.DataFrame(data=X.toarray(), columns=all_trigrams)\n",
    "train_min = train_features.min() # najmniejsza wartość z każdej kolumny\n",
    "train_max = train_features.max() # największa wartość z każdej kolumny\n",
    "train_features = (train_features - train_min)/(train_max-train_min) # do poprawy bo zwraca NaN, jeśli max value = 0\n",
    "train_features[\"lang\"] = list(results[\"lang\"]) # dodajemy dodatkową kolumnę z naszym outputem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e93ed2",
   "metadata": {},
   "source": [
    "### Asercje czy wszystko przebiegło pomyślnie, czy w naszym zbiorze nie ma NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5c5562f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "assert 0 not in train_max, \"Jeden z najczęściej występujących trigramów nie wystąpił ani razu\"\n",
    "assert 0 not in (train_max-train_min), \"Nie można dzielić przez 0\"\n",
    "assert not train_max.isnull().values.any(),  \"NaN w wektorze train_max\"\n",
    "assert not train_min.isnull().values.any(), \"NaN w wektorze train_min\"\n",
    "assert not (train_max-train_min).isnull().values.any(), \"NaN w mianowniku train_min\"\n",
    "assert not train_features.isnull().values.any(), \"NaN w wynikowym DataFrame\"\n",
    "\n",
    "#display(X)\n",
    "#display(train_features)\n",
    "#display(train_langs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5607a24",
   "metadata": {},
   "source": [
    "### Przygotowanie danych testowych i treningowych "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b7402d6",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, train_langs, test_size = 0.2)\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(languages)\n",
    "with open('encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(encoder, f)\n",
    "x = train_features.drop('lang',axis=1)\n",
    "y = encode(train_features['lang'], encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe03d68",
   "metadata": {},
   "source": [
    "### Przygotowanie modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05b1af3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(500, input_dim=len(x.columns), activation='relu'))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfeacce",
   "metadata": {},
   "source": [
    "### INFO korzystania z GPU lub CPU, można zakomentować jeśli nie chce się korzystać z GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "016b93ce",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 7117482110917930719\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3667263488\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 1236265035702339401\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      "]\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "------------------------------------------------------------------------------------------\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "from keras import backend as K\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "K._get_available_gpus()\n",
    "\n",
    "print(tf.config.experimental.list_physical_devices())\n",
    "\n",
    "print(\"------------------------------------------------------------------------------------------\")\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  print(physical_devices)\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass\n",
    "\n",
    "#Train model\n",
    "# Create a MirroredStrategy.\n",
    "tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc03e836",
   "metadata": {},
   "source": [
    "### Uruchomienie treningu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61f3bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 36864/133334 [=======>......................] - ETA: 6:26 - loss: 0.0647 - accuracy: 0.9762"
     ]
    }
   ],
   "source": [
    "# Z użyciem gpu\n",
    "with tf.device('/GPU:0'):\n",
    "    model.fit(x, y, epochs=1, batch_size=6)\n",
    "    \n",
    "# Bez użycia gpu\n",
    "#model.fit(x, y, epochs=30, batch_size=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987ff0cf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### Test precyzji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d01777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=model.predict(X_test)\n",
    "predictions = [encoder.classes_[np.argmax(label)] for label in labels]\n",
    "accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c7f1bc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"trigrams_recognition\")\n",
    "del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}